<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>
    Lista modeli z ü§ó - Modele z ü§ó
    
  </title>

  <meta name="description" content="by Jakub Pniewski">

  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/posts/">
  <link rel="alternate" type="application/rss+xml" title="Modele z ü§ó" href="/feed.xml">

</head>


<body>

  <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="/">Modele z ü§ó</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/posts">Posts</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>


  <!-- Page Header -->

  <header class="masthead">
    
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="page-heading">
            <h1>Lista modeli z ü§ó</h1>
            
          </div>
        </div>
      </div>
    </div>
  </header>

  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">

        <h2 id="deepseek-aideepseek-r1">deepseek-ai/DeepSeek-R1</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: mit</p>

<p><strong>Likes</strong>: 10.9k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1">ü§ó link</a></p>

<p><a href="/models/deepseek-ai_DeepSeek-R1">Post</a></p>

<p><strong>Short summary</strong>:
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences.</p>

<h2 id="black-forest-labsflux1-dev">black-forest-labs/FLUX.1-dev</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: flux-1-dev-non-commercial-license</p>

<p><strong>Likes</strong>: 9.21k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">ü§ó link</a></p>

<p><a href="/models/black-forest-labs_FLUX.1-dev">Post</a></p>

<p><strong>Short summary</strong>:
FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. It is available for personal, scientific, and commercial purposes.</p>

<h2 id="compvisstable-diffusion-v1-4">CompVis/stable-diffusion-v1-4</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: creativeml-openrail-m</p>

<p><strong>Likes</strong>: 6.71k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/CompVis/stable-diffusion-v1-4">ü§ó link</a></p>

<p><a href="/models/CompVis_stable-diffusion-v1-4">Post</a></p>

<p><strong>Short summary</strong>:
Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder.</p>

<h2 id="stabilityaistable-diffusion-xl-base-10">stabilityai/stable-diffusion-xl-base-1.0</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: openrail++</p>

<p><strong>Likes</strong>: 6.38k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">ü§ó link</a></p>

<p><a href="/models/stabilityai_stable-diffusion-xl-base-1.0">Post</a></p>

<p><strong>Short summary</strong>:
SDXL consists of an ensemble of experts pipeline for latent diffusion. In a first step, the base model is used to generate (noisy) latents. These latents are then further processed with a refinement model.</p>

<h2 id="meta-llamameta-llama-3-8b">meta-llama/Meta-Llama-3-8B</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: llama3</p>

<p><strong>Likes</strong>: 6.07k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">ü§ó link</a></p>

<p><a href="/models/meta-llama_Meta-Llama-3-8B">Post</a></p>

<p><strong>Short summary</strong>:
Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences</p>

<h2 id="bigsciencebloom">bigscience/bloom</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: bigscience-bloom-rail-1.0</p>

<p><strong>Likes</strong>: 4.86k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/bigscience/bloom">ü§ó link</a></p>

<p><a href="/models/bigscience_bloom">Post</a></p>

<p><strong>Short summary</strong>:
BLOOM is an autoregressive Large Language Model (LLM) trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. It is able to output coherent text in 46 languages and 13</p>

<h2 id="stabilityaistable-diffusion-3-medium">stabilityai/stable-diffusion-3-medium</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: stabilityai-ai-community</p>

<p><strong>Likes</strong>: 4.71k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/stabilityai/stable-diffusion-3-medium">ü§ó link</a></p>

<p><a href="/models/stabilityai_stable-diffusion-3-medium">Post</a></p>

<p><strong>Short summary</strong>:
Stable Diffusion 3 Medium is a Multimodal Diffusion Transformer (MMDiT) text-to-image model. It features greatly improved performance in image quality, typography, complex prompt understanding, and resource-</p>

<h2 id="mistralaimixtral-8x7b-instruct-v01">mistralai/Mixtral-8x7B-Instruct-v0.1</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: apache-2.0</p>

<p><strong>Likes</strong>: 4.33k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">ü§ó link</a></p>

<p><a href="/models/mistralai_Mixtral-8x7B-Instruct-v0.1">Post</a></p>

<p><strong>Short summary</strong>:
The Mixtral-8x7B Large Language Model (LLM) is a pretrained Sparse Mixture of Experts. The model outperforms Llama 2 70B on most benchmarks.</p>

<h2 id="meta-llamallama-2-7b-chat-hf">meta-llama/Llama-2-7b-chat-hf</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: llama2</p>

<p><strong>Likes</strong>: 4.28k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">ü§ó link</a></p>

<p><a href="/models/meta-llama_Llama-2-7b-chat-hf">Post</a></p>

<p><strong>Short summary</strong>:
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Llama-2-Chat models outperform open-source chat models on most benchmarks</p>

<h2 id="meta-llamallama-2-7b">meta-llama/Llama-2-7b</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: llama2</p>

<p><strong>Likes</strong>: 4.27k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/meta-llama/Llama-2-7b">ü§ó link</a></p>

<p><a href="/models/meta-llama_Llama-2-7b">Post</a></p>

<p><strong>Short summary</strong>:
Llama 2 is a collection of pretrained and fine-tuned generative text models. Llama-2-Chat models outperform open-source chat models on most benchmarks.</p>

<h2 id="openaiwhisper-large-v3">openai/whisper-large-v3</h2>

<p><strong>Type</strong>: Automatic Speech Recognition</p>

<p><strong>License</strong>: apache-2.0</p>

<p><strong>Likes</strong>: 4.13k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/openai/whisper-large-v3">ü§ó link</a></p>

<p><a href="/models/openai_whisper-large-v3">Post</a></p>

<p><strong>Short summary</strong>:
Whisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation. Whisper large-v3 model was trained on 1 million hours of weakly labeled audio and 4 million hours</p>

<h2 id="stabilityaistable-diffusion-2-1">stabilityai/stable-diffusion-2-1</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: openrail++</p>

<p><strong>Likes</strong>: 3.95k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">ü§ó link</a></p>

<p><a href="/models/stabilityai_stable-diffusion-2-1">Post</a></p>

<p><strong>Short summary</strong>:
This model card focuses on the model associated with the Stable Diffusion v2-1 model. The model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional</p>

<h2 id="meta-llamameta-llama-3-8b-instruct">meta-llama/Meta-Llama-3-8B-Instruct</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: llama3</p>

<p><strong>Likes</strong>: 3.85k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">ü§ó link</a></p>

<p><a href="/models/meta-llama_Meta-Llama-3-8B-Instruct">Post</a></p>

<p><strong>Short summary</strong>:
Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences</p>

<h2 id="warriormama777orangemixs">WarriorMama777/OrangeMixs</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: creativeml-openrail-m</p>

<p><strong>Likes</strong>: 3.81k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/WarriorMama777/OrangeMixs">ü§ó link</a></p>

<p><a href="/models/WarriorMama777_OrangeMixs">Post</a></p>

<p><strong>Short summary</strong>:
‚ÄúOrangeMixs‚Äù shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others. Maintain a repository for the following purposes.</p>

<h2 id="lllyasvielcontrolnet-v1-1">lllyasviel/ControlNet-v1-1</h2>

<p><strong>Type</strong>: Updated
					Apr 25, 2023</p>

<p><strong>License</strong>: openrail</p>

<p><strong>Likes</strong>: 3.75k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/lllyasviel/ControlNet-v1-1">ü§ó link</a></p>

<p><a href="/models/lllyasviel_ControlNet-v1-1">Post</a></p>

<p><strong>Short summary</strong>:
This model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.</p>

<h2 id="meta-llamallama-31-8b-instruct">meta-llama/Llama-3.1-8B-Instruct</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: llama3.1</p>

<p><strong>Likes</strong>: 3.71k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">ü§ó link</a></p>

<p><a href="/models/meta-llama_Llama-3.1-8B-Instruct">Post</a></p>

<p><strong>Short summary</strong>:
The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out)</p>

<h2 id="lllyasvielcontrolnet">lllyasviel/ControlNet</h2>

<p><strong>Type</strong>: Updated
					Feb 25, 2023</p>

<p><strong>License</strong>: openrail</p>

<p><strong>Likes</strong>: 3.67k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/lllyasviel/ControlNet">ü§ó link</a></p>

<p><a href="/models/lllyasviel_ControlNet">Post</a></p>

<p><strong>Short summary</strong>:
The ControlNet+SD1.5 model to control SD using canny edge detection. The model is trained with boundary edges with very strong data augmentation to simulate boundary lines similar to that drawn by human.</p>

<h2 id="mistralaimistral-7b-v01">mistralai/Mistral-7B-v0.1</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: apache-2.0</p>

<p><strong>Likes</strong>: 3.63k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">ü§ó link</a></p>

<p><a href="/models/mistralai_Mistral-7B-v0.1">Post</a></p>

<p><strong>Short summary</strong>:
Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. It outperforms Llama 2 13B on all benchmarks we tested.</p>

<h2 id="deepseek-aideepseek-v3">deepseek-ai/DeepSeek-V3</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: ND</p>

<p><strong>Likes</strong>: 3.6k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3">ü§ó link</a></p>

<p><a href="/models/deepseek-ai_DeepSeek-V3">Post</a></p>

<p><strong>Short summary</strong>:
DeepSeek-V3 is a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. It adopts Multi-head Latent Attention (MLA</p>

<h2 id="hexgradkokoro-82m">hexgrad/Kokoro-82M</h2>

<p><strong>Type</strong>: Text-to-Speech</p>

<p><strong>License</strong>: apache-2.0</p>

<p><strong>Likes</strong>: 3.57k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/hexgrad/Kokoro-82M">ü§ó link</a></p>

<p><a href="/models/hexgrad_Kokoro-82M">Post</a></p>

<p><strong>Short summary</strong>:
Kokoro is an open-weight TTS model with 82 million parameters. It delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production</p>

<h2 id="black-forest-labsflux1-schnell">black-forest-labs/FLUX.1-schnell</h2>

<p><strong>Type</strong>: Text-to-Image</p>

<p><strong>License</strong>: apache-2.0</p>

<p><strong>Likes</strong>: 3.48k</p>

<p><strong>Transformers</strong>: ‚ùå</p>

<p><a href="https://huggingface.co/black-forest-labs/FLUX.1-schnell">ü§ó link</a></p>

<p><a href="/models/black-forest-labs_FLUX.1-schnell">Post</a></p>

<p><strong>Short summary</strong>:
FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. Released under the apache-2.0 licence, the model can be used for personal, scientific,</p>

<h2 id="microsoftphi-2">microsoft/phi-2</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: mit</p>

<p><strong>Likes</strong>: 3.28k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/microsoft/phi-2">ü§ó link</a></p>

<p><a href="/models/microsoft_phi-2">Post</a></p>

<p><strong>Short summary</strong>:
Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites</p>

<h2 id="deepseek-aijanus-pro-7b">deepseek-ai/Janus-Pro-7B</h2>

<p><strong>Type</strong>: Any-to-Any</p>

<p><strong>License</strong>: mit</p>

<p><strong>Likes</strong>: 3.18k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B">ü§ó link</a></p>

<p><a href="/models/deepseek-ai_Janus-Pro-7B">Post</a></p>

<p><strong>Short summary</strong>:
Janus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways. The simplicity, high flexibility, and effectiveness of</p>

<h2 id="googlegemma-7b">google/gemma-7b</h2>

<p><strong>Type</strong>: Text Generation</p>

<p><strong>License</strong>: gemma</p>

<p><strong>Likes</strong>: 3.13k</p>

<p><strong>Transformers</strong>: ‚úÖ</p>

<p><a href="https://huggingface.co/google/gemma-7b">ü§ó link</a></p>

<p><a href="/models/google_gemma-7b">Post</a></p>

<p><strong>Short summary</strong>:
Gemma is a family of lightweight, state-of-the-art open models from Google. Built from the same research and technology used to create Gemini models. Text-to-text, decoder-only large language models</p>


      </div>
    </div>
  </div>


  <!-- Footer -->

<hr>

<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ul class="list-inline text-center">
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/xweinp">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
        </ul>
        <p class="copyright text-muted">Copyright &copy;  2025</p>
      </div>
    </div>
  </div>
</footer>


  <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="/assets/vendor/startbootstrap-clean-blog/js/scripts.js"></script>

<script src="/assets/scripts.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>



</body>

</html>
